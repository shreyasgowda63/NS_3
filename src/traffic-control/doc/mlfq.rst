.. include:: replace.txt
.. highlight:: bash

Mlfq queue disc
---------------------

Model Description
*****************

MlfqQueueDisc implements a Multi-Level Feedback Queue (MLFQ) policy originating from operating systems
([Ref1]_) to mimic Shortest Job First algorithm at the traffic control layer of the traffic senders, 
where packets are tagged based on the historically transmitted flow size that they belong to. Packets 
from the same flow are tagged first with the top priority, and as the transmitted flow size in bytes 
increases and hit the threshold value for the next lower priority, the priority would be demoted to the 
next lower priority. The tagged packet will be enqueued to the corresponding First In, First Out (FIFO) 
queue and the strict priority policy (i.e. the packet from the highest priority queue will be dequeued 
and transmitted first) is implemented on these service queues for the packet transmission.

MlfqQueueDiscs are required to install at network source hosts and PrioQueueDiscs need to be installed 
at network switches in order to fully implement the MLFQ queueing policy. The packets tagged 
at the network source hosts with MlfqQueueDiscs would be equipped with the priority value 
that will be leveraged to classify the packet to the corresponding FIFO queue at network 
switches. Packets are served for transmission based on the strict priority policy enforced 
at the PrioQueueDisc. Notice that the implementation assumes that the maximum number of 
priorities supported is 16 to be compatible with the existing PrioQueueDisc implementation 
and the commodity switches. The destination nodes might send short flows with only signal packets 
backward (e.g., ACK) in bi-directional communication with a different flow signature from that of 
forward-direction flow. The priority of the reverse flow does not need to match the priority of the
forward-direction flow. Meanwhile, one could also install MlfqQueueDisc at network destination hosts
to enforce the scheduling policies on these flows as well.

The figure shows an example packet path during which the packets generated by the 
source application pass through the protocol stack and are inserted with FlowPriorityTag
with the priority information at the traffic control layer (MlfqQueueDisc). The packets
are then forwarded for transmission and arrive at the network switches. The switches include
the FlowPrioPacketFilter which would classify the arriving packets based on the priority 
information and enqueue them to the corresponding bands. Packets queueing at these bands 
are served with the strict priority policy for transmission until they arrive at the destination.

.. _mlfq-packet-path:

.. figure:: figures/mlfq-packet-path.*
   :align: center

   Example packet path with MLFQ scheduling.

If no queue disc class is added by the user before the queue disc is initialized, two child 
queue discs of type FifoQueueDisc are automatically included. Meanwhile, MlfqQueueDisc needs 
at least two child queue discs during configuration.

Notes
==========

We implement a custom FlowPriorityTag to store the tagging information. The original proposal 
([Ref2]_) of using MLFQ for flow scheduling in Data Center Networking mentions the usage of 
DSCP field in IPv4 header for storing the tagging information, however, since MlfqQueueDisc 
is a generic scheduling method to mimic Shortest Job First algorithm and the tagging method 
is not standardized, we use the custom packet tag as the flexible tagging solution.

We also create an accompanying FlowPrioPacketFilter to be included during the instantiating
of PrioQueueDisc that would read the priority value in the FlowPriorityTag during the packet 
classification. It will return the non-negative value ``i`` that is less than the number of 
priority bands (one has to make sure that the number of bands configured at PrioQueueDisc 
is equal to numPriority configured at the MlfqQueueDisc). Then the packet is enqueued into
the ``i``-th priority band. The introduction of FlowPrioPacketFilter will avoid changing 
the existing PrioQueueDisc DoEnqueue method since PrioQueueDisc DoEnqueue is compatible 
with the installed custom PacketFilter.

The MlfqQueueDisc maintains a hash table which maps the hashing value of the packet (differentiating 
the flow it belongs to) to the number of bytes transmitted historically. The maximum number
of bytes in the table is UINT64_MAX (~10^10 GB). To avoid the starvation of long flows, the size value is reset 
to zero by the ResetThreshold. Notice that if the flow is completed, the entry will still remain in the hash table. 
This could be a performance issue in the large scale simulation where the number of flows is large AND dynamic. 
In the future, we could either set a timer to delete the entry when it is expired or let the upper layer protocol
used by the flow to send a notification to traffic control layer.

Notice that MlfqQueueDisc implementation does not restrict its usage to specific queue disc
items such as Ipv4QueueDiscItem, Ipv6QueueDiscItem, and ArpQueueDiscItem. However, to allow 
differentiating multiple flows that packets belong to, the user needs to make sure the queue disc
item used overloads the Hash() function defined in QueueDiscItem class. Otherwise, it always returns
0 and packets will only get one-class priority service -- all flows have the same priority.

Perturbation is an optional configuration attribute and can be used to generate different hash 
outcomes for different inputs. For instance, the tuples used as input to the hash may cause 
hash collisions (mapping to the same flow) for a given set of inputs, but by changing the 
perturbation value, the same hash inputs now map to distinct flows.

References
==========
.. [Ref1] Multi-Level Feedback Queue Tutorial at `<http://pages.cs.wisc.edu/~remzi/OSTEP/cpu-sched-mlfq.pdf>`_.
.. [Ref2] Bai, Wei, et al. "Information-agnostic flow scheduling for commodity data centers." 
12th {USENIX} Symposium on Networked Systems Design and Implementation ({NSDI} 15) at 
`<http://sing.cse.ust.hk/papers/pias-nsdi2015.pdf>`_.

Attributes
==========

The MlfqQueueDisc class holds the following attribute:

* ``ThresholdVector:`` The Flow size threshold vector (in bytes) configured for the priority set.
The default value is ThresholdVector{20000} corresponding to 2 priorities supported.
* ``NumPriority:`` Number of priorities supported (max 16). The default value is 2.
* ``ResetThreshold:`` Flow size threshold (in bytes) to reset the transmitted bytes to prevent
the starvation of long flows. The default value is 15000000.
* ``Perturbation:`` The salt used as an additional input to the hash function used to classify
packets. The default value is 0.

Examples
========

An example of how to configure MlfqQueueDisc and PrioQueueDisc to fully support the MLFQ scheduling
is `mlfq-example.cc` located in ``src/traffic-control/examples``. 

The simulation of MLFQ requires compatible PrioQueueDiscs at switches (i.e., the number of priorities should
equal to that of MlfqQueueDisc and FlowPrioPacketFilter should be installed). Any mismatch might lead 
to degradation of MLFQ scheduling. For example, the negative scenarios below would lead to the deviation from
MLFQ scheduling (though the program will not crash under these mismatched configurations):

1. If the end host does not install MlfqQueueDisc (e.g., all nodes are configured only with PrioQueueDisc).
Flows will not be tagged with the right priority based on their flow sizes hence MLFQ will not be executed.

2. If the end hosts install MlfqQueueDisc while switches install neither PrioQueueDisc nor FlowPrioPacketFilter,
the tagged packets by MlfqQueueDisc could not be classified to different FIFO queues where the strict priority policy 
is enforced. In this case, though packets from different flows are marked with the corresponding priority, such 
metadata is not leveraged during packet scheduling at network switches, hence, MLFQ is only partially utilized at 
the network edges.

3. If the end hosts install MlfqQueueDisc while switches install PrioQueueDisc but not FlowPrioPacketFilter, 
PrioQueueDisc could not interpret the priority information in the packet during DoEnqueue() and therefore the packets
are not served based on the priority information. MLFQ is stll only partially utilized at the network edges.

4. If the end hosts install MlfqQueueDisc while switches install FlowPrioPacketFilter and non-PrioQueueDisc, 
MLFQ is stll only partially utilized at the network edges. Only PrioQueueDisc among existing QueueDiscs leverages the 
installed PacketFilter while calling Classify() to classify packets into the corresponding service bands, the existing 
non-PrioQueueDisc either does not call PacketFilter for packet classification or call the PacketFilter for other purposes 
(e.g., FqCoDelFlow calls Classify() method for deciding the flow hash value). Both cases would not lead to program crashes
but is not sufficient to actually utilize MLFQ for packet scheduling at the network switches.

.. sourcecode:: bash

   $ ./waf configure --enable-examples
   $ ./waf --run "mlfq-example --PrintHelp"
   $ # Run MlfqQueueDisc
   $ NS_LOG="MlfqSjfExample" ./waf --run "mlfq-sjf-example --queueDiscName=MlfqQueueDisc"
   $ # Run FifoQueueDisc
   $ NS_LOG="MlfqSjfExample" ./waf --run "mlfq-sjf-example --queueDiscName=FifoQueueDisc"

Validation
**********

MlfqQueueDisc is tested using :cpp:class:`MlfqQueueDiscTestSuite` class defined in 
``src/traffic-control/test/mlfq-queue-disc-test-suite.cc``. It includes 6 test cases:

* Test 1: The custom ThresholdVector attribute could be set correctly.
* Test 2: Packets of the same flow are tagged and enqueued correctly based on the ThresholdVector.
* Test 3: ResetThreshold is working correctly for a simulated long flow.
* Test 4: Strict priority policy is correctly enforced during DoDequeue.
* Test 5: Combined with PrioQueueDisc, the tagged priority could be leveraged for classification.

The test suite can be run using the following commands:

.. sourcecode:: bash

  $ ./waf configure --enable-tests
  $ ./waf build
  $ ./test.py -s mlfq-queue-disc

or

.. sourcecode:: bash

  $ ./waf configure --enable-tests
  $ NS_LOG="MlfqQueueDisc" ./waf --run "test-runner --suite=mlfq-queue-disc"